这是一个赌徒问题的实现，实际上他仍然是一个有限MDP过程，前面的投篮机器人也是使用了DP方法。
但在这个问题中，我们关注的不是怎么用DP解决某个有限MDP问题，而是为了区分值迭代和策略迭代的不同之处。
对于值迭代，会直接更新每个状态可能达到的最大的状态价值，因此策略自然地就会选择贪心策略，也就是说，在状态价值估计完的时候，动作就已经确定了。
但这里的状态价值并非真实价值，而是已经确定了动作的，因此称其为状态动作价值似乎更为合理。

而对于策略迭代，是指先随机生成一个策略，然后对该策略进行评估，评估时需要用到下一状态的状态价值，这里的状态价值使用期望来表示的，因此是真实的状态价值。
在评估完之后，策略提升会通过遍历实现将这个策略施行后每一个状态中选择的动作的动作价值，和这个状态下的最优动作价值的对比，然后择优替代，最终得到最优策略。